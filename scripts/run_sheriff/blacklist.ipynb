{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bb88a4-1d49-4c8b-8246-217685670a58",
   "metadata": {},
   "source": [
    "# Blacklist\n",
    "This notebook was used to generate a blacklist of genome regions that contain likely mapping artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f022c-8309-4b12-9ae5-987fe49dd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8efd20-4a09-46e9-9839-d746c9ad070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter input bam by split-pipe sample id(s)\n",
    "import os\n",
    "import glob\n",
    "import pysam\n",
    "\n",
    "def filter_bam_by_spipe_sample_id(input_dir, bam_file_pattern, sample_ids, output_dir):\n",
    "    \"\"\"\n",
    "    Filters BAM files in a directory based on sample IDs extracted from cell barcode tags and writes the output to a specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): Path to the directory containing input BAM files.\n",
    "        bam_file_pattern (str): Pattern to match BAM filenames (e.g., \"*.bam\").\n",
    "        sample_ids (list of str): List of sample IDs to filter for (e.g., ['01', '02']).\n",
    "        output_dir (str): Path to the output directory.\n",
    "    \"\"\"\n",
    "    # Ensure the BAM file pattern ends with \"*.bam\" if not already specified\n",
    "    if not bam_file_pattern.endswith(\".bam\"):\n",
    "        bam_file_pattern += \"*.bam\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find BAM files matching the pattern in the input directory\n",
    "    input_bam_files = glob.glob(os.path.join(input_dir, bam_file_pattern))\n",
    "\n",
    "    if not input_bam_files:\n",
    "        print(f\"No BAM files found in directory '{input_dir}' with pattern '{bam_file_pattern}'\")\n",
    "        return\n",
    "\n",
    "    for input_bam in input_bam_files:\n",
    "        try:\n",
    "            # Construct the output BAM filename\n",
    "            input_filename = os.path.basename(input_bam)\n",
    "            base_name = os.path.splitext(input_filename)[0]\n",
    "            output_filename = f\"{base_name}_samples_{sample_ids[0]}_to_{sample_ids[-1]}.bam\"\n",
    "            output_bam = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Open the input BAM file\n",
    "            with pysam.AlignmentFile(input_bam, \"rb\") as bamfile:\n",
    "                # Open the output BAM file\n",
    "                with pysam.AlignmentFile(output_bam, \"wb\", header=bamfile.header) as outfile:\n",
    "                    # Iterate through each read in the BAM file\n",
    "                    for read in bamfile:\n",
    "                        # Check if the read has the cell barcode tag\n",
    "                        if read.has_tag(\"CB\"):\n",
    "                            # Extract the cell barcode\n",
    "                            cell_barcode = read.get_tag(\"CB\")\n",
    "                            # Split the cell barcode to get the sample ID\n",
    "                            sample_id = cell_barcode.split('_')[0]\n",
    "                            # Check if the sample ID is in the list of desired sample IDs\n",
    "                            if sample_id in sample_ids:\n",
    "                                # Write the read to the output BAM file\n",
    "                                outfile.write(read)\n",
    "            \n",
    "            print(f\"Filtered BAM file saved to: {output_bam}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing BAM file '{input_bam}': {e}\")\n",
    "\n",
    "# Example usage\n",
    "# input_dir = \"/path/to/input_directory\"\n",
    "# bam_file_pattern = \"*.bam\"\n",
    "# sample_ids = [\"01\", \"02\", \"03\"]  # Replace with the desired sample IDs\n",
    "# output_dir = \"/path/to/output_directory\"\n",
    "# filter_bam_by_spipe_sample_id(input_dir, bam_file_pattern, sample_ids, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b49b6f-6bb1-4926-a354-ba5ddd0ee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = # OMITTED\n",
    "# Split-pipe aligned reads, 10k cell library, downsampled 100x\n",
    "spipe_100x_down_bam = # OMITTED\n",
    "sample_ids = ['01', '02']\n",
    "output_dir = # OMITTED\n",
    "\n",
    "filter_bam_by_spipe_sample_id(input_dir, spipe_100x_down_bam, sample_ids, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280fd5f-7948-497a-bc60-2990e63be80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert genome file to parse chrom names\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def genome_to_parse_chroms(input_genome_file, output_dir):\n",
    "    # Read the input genome file\n",
    "    genome_df = pd.read_csv(input_genome_file, sep='\\t', header=None)\n",
    "    \n",
    "    # Perform the string processing on the first column\n",
    "    genome_df[0] = genome_df[0].str.replace('chr', 'hg38_')\n",
    "    genome_df[0] = genome_df[0].str.replace(r'_(.*?)_G', '_G', regex=True)\n",
    "    genome_df[0] = genome_df[0].str.replace(r'_(.*?)_K', '_K', regex=True)\n",
    "    genome_df[0] = genome_df[0].str.replace('Un_', '')\n",
    "    genome_df[0] = genome_df[0].str.replace('v', '.')\n",
    "    genome_df[0] = genome_df[0].str.replace('_alt', '')\n",
    "    genome_df[0] = genome_df[0].str.replace('_random', '')\n",
    "\n",
    "    # Extract the input filename and construct the output filename\n",
    "    input_filename = os.path.basename(input_genome_file)\n",
    "    output_filename = input_filename.replace('.genome', '_parse_chroms.genome')\n",
    "    output_file_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    # Write the processed data to the output file\n",
    "    genome_df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "    print(f\"Processed genome file saved to: {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "# genome_to_parse_chroms('input.genome', 'output_directory')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3c65e-f45c-4be0-8727-fc740f1a62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_genome = # OMITTED\n",
    "output_dir = # OMITTED\n",
    "\n",
    "genome_to_parse_chroms(input_genome, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286cf81-4bed-4a47-bcaa-81d468bcc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calls (blacklist) peaks from input bam\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def bam_to_blacklist_peaks(bam_file, output_dir, output_prefix=\"\"):\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Define MACS2 command   \n",
    "    # --nomodel,       Skip dynamic lambda shifting model\n",
    "    # --extsize 98,    Manual alignment length for pileup when --nomodel set, 3' extension from 5' shift position\n",
    "    # --max-gap 3000,  Peaks within this distance will be merged\n",
    "    # --keep-dup all,  Keep all reads\n",
    "    # -q 1e-50, Q-val  sig cutoff for peaks\n",
    "    # -g hs,           Human genome size, for calculating background signal\n",
    "    # -n,              Prefix for output files\n",
    "    # -t,              Treatment bam\n",
    "    # --outdir,        Output directory\n",
    "    # -B --SPMR,       Output bedGraphs (fragment pileup per M reads)\n",
    "    # --trackline,     Include UCSC genome browser header line in outputs\n",
    "    macs2_command = f\"macs2 callpeak --nomodel --extsize 98 --max-gap 3000 --keep-dup all -q 1e-50 -g hs -n {output_prefix} -t {bam_file} --outdir {output_dir} -B --SPMR --trackline\"\n",
    "    \n",
    "    # Run MACS2 command\n",
    "    subprocess.run(macs2_command, shell=True, check=True)\n",
    "\n",
    "    # File paths for narrowPeak calling\n",
    "    bed_file = os.path.join(output_dir, f'{output_prefix}_summits.bed')\n",
    "    narrowpeak_file = os.path.join(output_dir, f'{output_prefix}_peaks.narrowPeak')\n",
    "    treatment_bedgraph_file = os.path.join(output_dir, f'{output_prefix}_treat_pileup.bdg')\n",
    "    control_bedgraph_file = os.path.join(output_dir, f'{output_prefix}_control_lambda.bdg')\n",
    "\n",
    "    # Function to process the file: replace strings, filter lines, and modify track name\n",
    "    def process_file(input_file, output_file, output_prefix):\n",
    "        input_filename = os.path.basename(input_file)\n",
    "        track_name = input_filename.split('_')[-1].split('.')[0]\n",
    "        new_track_name = f\"{output_prefix}_{track_name}\"\n",
    "\n",
    "        with open(input_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        processed_lines = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0 and line.startswith('track'):\n",
    "                # Replace the entire name field in the trackline\n",
    "                line = line.replace('name=\"', f'name=\"{new_track_name}\"')\n",
    "                processed_lines.append(line)\n",
    "                continue\n",
    "            # Split the line into columns\n",
    "            columns = line.strip().split('\\t')\n",
    "            if not (columns[0].startswith('hg38_G') or columns[0].startswith('hg38_K')):\n",
    "                processed_line = line.replace('hg38_', 'chr').replace('chrMT', 'chrM')\n",
    "                processed_lines.append(processed_line)\n",
    "\n",
    "        with open(output_file, 'w') as file:\n",
    "            file.writelines(processed_lines)\n",
    "\n",
    "    # Process and write new files with \"ucsc\" appended\n",
    "    process_file(bed_file, os.path.join(output_dir, f'{output_prefix}_summits_ucsc.bed'), output_prefix)\n",
    "    process_file(narrowpeak_file, os.path.join(output_dir, f'{output_prefix}_peaks_ucsc.narrowPeak'), output_prefix)\n",
    "    process_file(treatment_bedgraph_file, os.path.join(output_dir, f'{output_prefix}_treat_pileup_ucsc.bdg'), output_prefix)\n",
    "    process_file(control_bedgraph_file, os.path.join(output_dir, f'{output_prefix}_control_lambda_ucsc.bdg'), output_prefix)\n",
    "\n",
    "    # Sort .bed file by the fifth column in descending order\n",
    "    sorted_bed_file = os.path.join(output_dir, f'{output_prefix}_summits_by_qval.bed')\n",
    "    bed_df = pd.read_csv(bed_file, sep='\\t', header=None, skiprows=1)\n",
    "    bed_df[4] = pd.to_numeric(bed_df[4], errors='coerce')\n",
    "    bed_df_sorted = bed_df.sort_values(by=4, ascending=False)\n",
    "    bed_df_sorted.to_csv(sorted_bed_file, sep='\\t', header=False, index=False)\n",
    "\n",
    "    # Sort .narrowPeak file by descending q-value 9th col (instead of fold enrich 7th col)\n",
    "    sorted_narrowpeak_file = os.path.join(output_dir, f'{output_prefix}_peaks_by_qval.narrowPeak')\n",
    "    narrowpeak_df = pd.read_csv(narrowpeak_file, sep='\\t', header=None, skiprows=1)\n",
    "    narrowpeak_df[8] = pd.to_numeric(narrowpeak_df[8], errors='coerce')\n",
    "    narrowpeak_df_sorted = narrowpeak_df.sort_values(by=8, ascending=False)\n",
    "    narrowpeak_df_sorted.to_csv(sorted_narrowpeak_file, sep='\\t', header=False, index=False)\n",
    "\n",
    "    # Print total number of peaks\n",
    "    total_peaks = len(narrowpeak_df_sorted)\n",
    "    print(f\"Total number of peaks: {total_peaks}\")\n",
    "\n",
    "    # Read the sorted bed file for q values\n",
    "    sorted_bed_df = pd.read_csv(sorted_bed_file, sep='\\t', header=None)\n",
    "    total_q_peaks = len(sorted_bed_df)\n",
    "\n",
    "    # Print the top three peaks with the strongest q values\n",
    "    print(\"Top 3 peaks with strongest q values:\")\n",
    "    for i in range(min(3, total_q_peaks)):\n",
    "        row = sorted_bed_df.iloc[i]\n",
    "        location = f\"{row[0]}:{row[1]}-{row[2]}\"\n",
    "        q_value = row[4]\n",
    "        print(f\"Peak {i+1}: Location = {location}, Q Value = {q_value}\")\n",
    "\n",
    "# Example usage\n",
    "# bam_to_blacklist_peaks('input.bam', 'output_directory')\n",
    "# bam_to_blacklist_peaks('input.bam', 'output_directory', 'output_prefix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c85723-19ac-4cd0-99cc-4d359c74327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-pipe aligned reads, 10k cell library, downsampled 100x, unedited sample\n",
    "# Generated with filter_bam_by_spipe_sample_id()\n",
    "spipe_100x_down_no_edit_bam = # OMITTED\n",
    "output_dir = # OMITTED\n",
    "\n",
    "bam_to_blacklist_peaks(spipe_100x_down_no_edit_bam, output_dir, output_prefix='black_100x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ff55d-9d90-4206-8f54-453b0916e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Code snippets then used to add the polyN sites >50bp in length, noting that additional sites were also added afterward.\n",
    "### Converting the narrowPeak file from the macs2 peak calling above to a standard bed file\n",
    "# cut -f1-6 black_100x_peaks_by_qval.narrowPeak > black_100x_peaks_by_qval.bed\n",
    "\n",
    "######## Downloading repeat masker simple repeat annotations from IGV host\n",
    "# wget https://s3.amazonaws.com/igv.org.genomes/hg38/rmsk/hg38_rmsk_Simple_repeat.bed.gz\n",
    "# gzip -d hg38_rmsk_Simple_repeat.bed.gz\n",
    "\n",
    "# ALSO subset the repeats to just those that have a >60 repeats, which likely effects mapping of the 100bp reads,\n",
    "# especially when they have the soft-clippped sequence and potentially the TSO!!\n",
    "# awk '($2 - $1) > 70' hg38_rmsk_Simple_repeat.bed > hg38_rmsk_Simple_repeat.70N.bed\n",
    "# awk '{gsub(/chr/, \"hg38_\"); print}' hg38_rmsk_Simple_repeat.70N.bed > hg38_rmsk_Simple_repeat.70N.renamed.bed\n",
    "# cat black_100x_peaks_by_qval.bed hg38_rmsk_Simple_repeat.70N.renamed.bed > black_100x_peaks_by_qval.simple_repeats_70N.bed\n",
    "#blacklist_file = f\"{data_dir}black_100x_peaks_by_qval.simple_repeats_70N.bed\"\n",
    "\n",
    "# Turns out this blacklist bed is WAYY to broad. Main issues seems to be the VERY simple repeats, like for example (T)n\n",
    "# Will just limit it to those cases!!!\n",
    "# awk '{gsub(/chr/, \"hg38_\"); print}' hg38_rmsk_Simple_repeat.bed > hg38_rmsk_Simple_repeat.renamed.bed\n",
    "# grep -E '\\(T\\)n|\\(A\\)n|\\(C\\)n|\\(G\\)n' hg38_rmsk_Simple_repeat.renamed.bed > hg38_rmsk_Simple_repeat.renamed.polyN.bed\n",
    "# awk -F'\\t' '($3 - $2) > 50' hg38_rmsk_Simple_repeat.renamed.polyN.bed > hg38_rmsk_Simple_repeat.renamed.polyN.50N.bed\n",
    "\n",
    "#### Check by printing diffs:\n",
    "# awk  -F'\\t' '{print $3 - $2}' hg38_rmsk_Simple_repeat.renamed.polyN.50N.bed\n",
    "\n",
    "### Make it so they are just 1 bp peaks, so need most of the simple repeat to overlap the edit region!\n",
    "# awk -F'\\t' '{print $1 \"\\t\" int($2 + (($3 - $2) / 2)) \"\\t\" int(($2 + (($3 - $2) / 2))+1)}' hg38_rmsk_Simple_repeat.renamed.polyN.50N.bed > hg38_rmsk_Simple_repeat.renamed.polyN.50N.1bp.bed\n",
    "# cat black_100x_peaks_by_qval.min.bed hg38_rmsk_Simple_repeat.renamed.polyN.50N.1bp.bed > black_100x_peaks_by_qval.simple_repeats_50N.bed\n",
    "\n",
    "# Renamed this to the chr convention, so can look at these in IGV:\n",
    "# awk '{gsub(\"hg38_\", \"chr\"); print}' black_100x_peaks_by_qval.simple_repeats_50N.bed > black_100x_peaks_by_qval.simple_repeats_50N.renamed.bed\n",
    "\n",
    "#### Now making adding the additional region which corresponded to a reference genome bias that looked like barcodes!:\n",
    "# This region added at top:\n",
    "# hg38_12 56112901    56112902\n",
    "# hg38_12 56112801    56113001\n",
    "# black_100x_peaks_by_qval.simple_repeats_50N.EXTRA.bed\n",
    "#\n",
    "# Renaming for visualisation purposes:\n",
    "# awk '{gsub(\"hg38_\", \"chr\"); print}' black_100x_peaks_by_qval.simple_repeats_50N.EXTRA.bed > black_100x_peaks_by_qval.simple_repeats_50N.EXTRA.renamed.bed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superbpy]",
   "language": "python",
   "name": "conda-env-superbpy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
